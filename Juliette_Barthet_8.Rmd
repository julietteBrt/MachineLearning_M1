---
title: "Week 8"
subtitle: "Gaussian Mixture Models & EM"
author: "Juliette Barthet"
date: "`r format(Sys.time())`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: cerulean
    highlight: espresso
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# EM Using mclust
## GMM vs. k-means
#### 1. Download and import Data1  and Data2 . Plot both of the datasets on the same window. Color the observations with respect to the ground truth.


```{r, figures-side, fig.show="hold", out.width="50%"}
require(gridExtra)


data1 <- read.csv("data1.csv", header=FALSE, row.names=NULL, sep=",")
data2 <- read.csv("data2.csv", header=FALSE, row.names=NULL, sep=",")

data1 <- data1[2:nrow(data1), ]
colnames(data1)<-c("X1", "X2", "truth")
data2 <- data2[2:nrow(data2), ]
colnames(data2)<-c("X1", "X2", "truth")
plot(data1$X1,data1$X2,col=data1$truth,pch=19,cex=1)
title(main = 'Data1')
plot(data2$X1,data2$X2,col=data2$truth,pch=19,cex=1)
title(main = 'Data2')
```

#### 2. Apply k-means on both datasets with 4 clusters. Plot both of the dataset on the same window and color the observations with respect to k-means results. Interpret the results.


```{r, fig.show="hold", out.width="50%" }
km1 <- kmeans(data1, centers = 4, iter.max = 20)
km2 <- kmeans(data2, centers = 4, iter.max = 20)

plot(data1$X1,data1$X2,col=km1$cluster,pch=19,cex=1)
title(main = 'Data1 with k-means, k = 4')

plot(data2$X1,data2$X2,col=km2$cluster,pch=19,cex=1)
title(main = 'Data2 with k-means, k = 4')
```

As we can see when comparing our data to the classification obtained with clustering, the first dataset has been correctly classified as the different classes are well separated. For the second dataset however, some points belonging to different classes but close to each others are wrongly classified since the classes are not as distinct as in the dataset 1.

#### 3. Now fit a GMM model on the datasets. To do so, load the mclust library. Then you can use the function Mclust() on your data (this function will choose automatically the number of mixtures, basing on BIC criterion). Use the clustering results from your GMM model to visualize the results on both of the datasets, color the observations with respect to the clusters obtained from the GMM model. Interpret the results.

```{r, fig.show="hold", out.width="50%" }
require(mclust)

mod1 <- Mclust(data1[,1:2])
mod2 <- Mclust(data2[, 1:2], G = 1:4)


plot(data1$X1,data1$X2,col=mod1$classification,pch=19,cex=1)
title(main = 'Data1 with Mclust')

plot(data2$X1,data2$X2,col=mod2$classification,pch=19,cex=1)
title(main = 'Data2 with Mclust')

```

The classification obrained with Mclust on the first dataset is accurate, the only misclassified points are the ones between the black and green clouds. Concerning the second dataset, we can see that since the classes are very close to each other it has troubles identifying 4 of them and misclassified a lot of points.

#### 4. Show the summary of the GMM model you fitted on Data2. Explain what it shows.

```{r}
summary(mod2)
```

From the summary, we learn that the Bayesian Information Criterion Value (BIC) is -5048.057. When choosing a model, we aim to take the one with the lowest BIC. We also get the log-likelihood corresponding to the BIC value. n is the number of observations in the data, here 400. df corresponds to the number of estimated parameters (35 here). The ICL (Integrated Complete Likelihood) value approximates the marginal likelihood (like BIC). Then we have the clustering table showing the different clusters and the number of observation forming them.

#### 5. mclust package offers some visualization. To plot your two-dimensional data, use the standard plot function applied on your model. Apply the following code, given that the model is named gmm_model, and interpret what it shows.

```{r, fig.show="hold", out.width="50%" }
plot(mod1, what = "classification")
title(main = 'Data1 classification')

plot(mod1, what = "uncertainty")
title(main = 'Data1 uncertainty')
```

The first plot shows the clustering.
The second plot shows the classification uncertainty. It highlights the uncertainty of a point to belong to the chosen class: the bigger the point, the higher the uncertainty.
On both plots we can see ellipses corresponding to covariances of mixture components.

```{r, fig.show="hold", out.width="50%" }
plot(mod2, what = "classification")
title(main = 'Data2 classification')

plot(mod2, what = "uncertainty")
title(main = 'Data2 uncertainty')
```

#### 6. mclust package uses the Bayesian Information Criterion (BIC) to choose the best number of mixtures. To see the values of BIC for different number of mixtures use the following code.

```{r, fig.show="hold", out.width="50%" }
plot(mod1, what = "BIC")
title(main = 'Data1 BIC values for different number of components')

plot(mod2, what = "BIC")
title(main = 'Data2 BIC values for different number of components')

```

On these plots are displayed the BIC values are different parameterisations of the within-group covariance matrix. We are using VVV method. Looking at the plots, we could say that 4 components is a suitable number for both of our datasets.

#### 7. Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data. Density estimation plays an important role in applied statistical data analysis and theoretical research. A density estimate based on GMM can be obtained using the function densityMclust(). Apply it on Data2 and visualize the estimated densities (show an “image” and a “perspective” plot of the bivariate density estimate).

```{r, fig.show="hold", out.width="50%" }
dens2 <- densityMclust(data2[,1:2])
plot(dens2, what = 'density',type='image')
title(main = 'Data2 density estimate image')

plot(dens2, what = 'density',type='persp')
title(main = 'Data2 density estimate perspective')
```

## EM on 1D

#### 8. Create a data table of 300 observations in which you have two columns:

* The first column contains generated data. Those data are generated from three Gaussian distributions with different parameters.
* The second column corresponds to the ground truth (every observation was generated from which Gaussian).
* Hint: functions you may need are rnorm(), rep(), rbind() or cbind().
* You must of course set a seed (your sutdent_pk).

```{r}
set.seed(702976)
gaussian1 <- rnorm(100, mean = -2, sd = 0.5)
gaussian2 <- rnorm(100, mean = 4, sd = 1.5)
gaussian3 <- rnorm(100, mean = 8, sd = 1)
data_gaussian <- cbind(X = c(gaussian1,gaussian2,gaussian3), source = c(rep(1, times= 100), rep(2, times=100), rep(3,times=100)))
```

#### 9. Show you generated data on one axe (this kind of figures are called stripchart), color them with respect to ground truth.

```{r}
require(lattice)
stripplot(data_gaussian[,1], col=data_gaussian[,2])

```

#### 10. Plot the histogram corresponding to your generated data. Interpret it.

```{r}
hist(data_gaussian[,1], xlim = c(-5, 12))
```

Looking at this histogram, we can see three different gaussian disctibutions superposed. Our data goes from approximately - 4 to  11 and we can see that we have high frequencies around the means defined earlier.

#### 11. Fit a GMM model on your generated data. Print the summary and visualize your results. Explain your results.

```{r}
require(mclust)

gaussian.gmm <- Mclust(data_gaussian[,1], g = 3)
summary(gaussian.gmm)
```

The GMM model fitted on our data identifies three different clusters of around 100 individuals each which corresponds to the data we generated.

```{r, fig.show="hold", out.width="50%" }

plot(gaussian.gmm, what = 'BIC')
plot(gaussian.gmm, what = 'uncertainty')
title(main = 'Gaussians uncertainty')
```

On the BIC plot, we can see that 3 is a suitable number of components since BIC value is the lowest for V method.
Also, we can see that the uncertainty is high for our red / green points. It is so because they were overlapping on our stipplot.

#### 12. Apply a density estimate on your generated data and visualize it. Interpret the obtained figure.

```{r, fig.show="hold", out.width="50%" }
gaussian.density <- densityMclust(data_gaussian[,1])
plot(gaussian.density, what = 'density',type = 'image')
title(main = 'Gaussian Data density estimate image')
```


The obtained figure corresponds to three gaussian distributions centered in - 2, 4 and 8, of standard deviations 0.5, 1.5 and 1.

## EM from Scratch

#### 2.1 Generate a two-dimensional dataset from a k-component Gaussian mixture density with different means and different covariance matrices. It is up to you to choose the mixing proportions
```{r}
g1 = c(rnorm(100,mean=-2,sd=2))
g2 = c(rnorm(100,mean=0,sd=0.5))
g3 = c(rnorm(100,mean=5,sd=1.5))
g4 = c(rnorm(100,mean=8,sd=3))

g5 = c(rnorm(100,mean=-4,sd=4))
g6 = c(rnorm(100,mean=10,sd=1))
g7 = c(rnorm(100,mean=-9,sd=2))
g8 = c(rnorm(100,mean=7,sd=5))

x1 = c(g1,g2,g3,g4)
x2 = c(g5,g6,g7,g8)

values = c(rep(1,100),rep(2,100),rep(3,100),rep(4,100))

data2D = cbind(x1,x2,values)

head(data2D)
plot(x1,x2,col=values,pch=20,
     xlab = 'X1',
     ylab = 'X2',
     main = 'Two-dimensional dataset')

```

